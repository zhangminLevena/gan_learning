{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self,depth=[1024,512,156,128],s_size=4):#每一层的filter的个数\n",
    "        self.depth=depth+[3]  #最后一层是RGB层\n",
    "        self.s_size=s_size\n",
    "        self.reuse=False#将该参数设置为true是指只能从已经创建过的变量进行读取\n",
    "    def __call__(self, inputs, training=False):\n",
    "        inputs=tf.convert_to_tensor(inputs)#将变量转为tensor格式\n",
    "        with tf.variable_scope('g',reuse=self.reuse):\n",
    "            with tf.variable_scope('reshape'):\n",
    "                outputs=tf.layers.dense(inputs,self.depth[0]*self.s_size*self.s_size)\n",
    "                outputs=tf.reshape(outputs,[-1,self.s_size,self.s_size,self.depth[0]])#将数据进行reshape\n",
    "                outputs=tf.nn.relu(tf.layers.batch_normalization(outputs,training=training),name=\"outputs\")\n",
    "            with tf.variable_scope('deconv1'):\n",
    "                outputs=tf.layers.conv2d_transpose(outputs,self.depth[1],[5,5],strides=(2,2),padding='SAME')#[5,5]的kernel size\n",
    "                outputs=tf.nn.relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('deconv2'):\n",
    "                outputs=tf.layers.conv2d_transpose(outputs,self.depth[2],[5,5],strides=(2,2),padding='SAME')#[5,5]的kernel size\n",
    "                outputs=tf.nn.relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('deconv3'):\n",
    "                outputs=tf.layers.conv2d_transpose(outputs,self.depth[3],[5,5],strides=(2,2),padding='SAME')#[5,5]的kernel size\n",
    "                outputs=tf.nn.relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('deconv4'):\n",
    "                outputs=tf.layers.conv2d_transpose(outputs,self.depth[4],[5,5],strides=(2,2),padding='SAME')#[5,5]的kernel size\n",
    "                outputs=tf.nn.relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('tanh'):\n",
    "                outputs=tf.tanh(outputs,name='outputs')\n",
    "        self.reuse=True\n",
    "        self.variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='g')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self,depth=[64,128,256,512]):\n",
    "        self.depth=[3]+depth\n",
    "        self.reuse=False\n",
    "    def __call__(self,inputs,training=False,name=''):\n",
    "        def leaky_relu(x,leaky=0.2,name=''):\n",
    "            return tf.maximum(x,x*leaky,name=name)#f(x)=αx，(x<0) f(x)=x，(x>=0)\n",
    "        \n",
    "        outputs=tf.convert_to_tensor(inputs)\n",
    "        with tf.name_scope('d'+name),tf.variable_scope('d',reuse=self.reuse):\n",
    "            with tf.variable_scope('conv1'):\n",
    "                outputs=tf.layers.conv2d(outputs,self.depth[1],[5,5],strides=(2,2),padding='SAME')\n",
    "                outputs=leaky_relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('conv2'):\n",
    "                outputs=tf.layers.conv2d(outputs,self.depth[2],[5,5],strides=(2,2),padding='SAME')\n",
    "                outputs=leaky_relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('conv3'):\n",
    "                outputs=tf.layers.conv2d(outputs,self.depth[3],[5,5],strides=(2,2),padding='SAME')\n",
    "                outputs=leaky_relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('conv4'):\n",
    "                outputs=tf.layers.conv2d(outputs,self.depth[4],[5,5],strides=(2,2),padding='SAME')\n",
    "                outputs=leaky_relu(tf.layers.batch_normalization(outputs,training=training),name='outputs')\n",
    "            with tf.variable_scope('classify'):\n",
    "                batch_size=outputs.get_shape()[0].value\n",
    "                reshape=tf.reshape(outputs,[batch_size,-1])\n",
    "                outputs=tf.layers.dense(reshape,2,name='outputs')\n",
    "        self.reuse=True\n",
    "        self.variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='d')\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    def __init__(self,\n",
    "                 batch_size=128,\n",
    "                 s_size=4,z_dim=100,g_depth=[1024,512,256,128],d_depth=[64,128,256,512]):\n",
    "        self.batch_size=batch_size\n",
    "        self.s_size=s_size\n",
    "        self.z_dim=z_dim\n",
    "        self.g=Generator(depth=g_depth,s_size=self.s_size)\n",
    "        self.d=Discriminator(depth=d_depth)\n",
    "        self.z=tf.random_uniform([self.batch_size,self.z_dim],minval=-1.0,maxval=1.0)\n",
    "    def loss(self,traindata):\n",
    "        generated=self.g(self.z,training=True)\n",
    "        g_outputs=self.d(generated,trainig=True,name='g')\n",
    "        t_outputs=self.d(traindata,training=True,name='t')\n",
    "        tf.add_to_collection('g_loss',tf._mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.ones([self.batch_size],dtype=tf.int64),\n",
    "                                                                                              logits=g_outputs)))#计算loss，用softmax来算\n",
    "        tf.add_to_collection('d_loss',tf.mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.ones([self.batch_size],dtype=tf.int64),\n",
    "                                                                                             logits=t_outputs)))\n",
    "        tf.add_to_collection('d_loss',tf.mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.zeros([self.batch_size],dtype=tf.int64),\n",
    "                                                                                             logits=t_outputs)))\n",
    "        return {\n",
    "            self.g:tf.add_n(tf.get_collection('g_loss'),name='total_g_loss'),#添加所有输入张量。\n",
    "            self.d:tf.add_n(tf.get_collection('d_loss'),name='total_d_loss')\n",
    "        }\n",
    "    def train(self,losses,learning_rate=0.0002,beta1=0.5):\n",
    "        g_opt=tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1)\n",
    "        d_opt=tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=beta1)#优化迭代操作\n",
    "        g_opt_op=g_opt.minimize(losses[self.g],var_list=self.g.variables)#最小化loss\n",
    "        d_opt_op=d_opt.minimize(losses[self.d],var_list=self.d.variables)#self.variables=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='g')\n",
    "        with tf.control_dependencies([g_opt_op,d_opt_op]):\n",
    "            return tf.no_op('name=train')\n",
    "        \n",
    "    def sample_images(self,row=8,col=8,inputs=None):\n",
    "        if inputs is None:\n",
    "            inputs=self.z\n",
    "        image=self.g(inputs,training=True)\n",
    "        image=tf.image.convert_image_dtype(tf.div(tf.add(image,1.0),2.0),tf.uint8)\n",
    "        image=[image for image in tf.split(image,self.batch_size,axis=0)]\n",
    "        rows=[]\n",
    "        for i in range(row):\n",
    "            rows.append(tf.concat(image[col*i+0:col*i+col],2))\n",
    "        image=tf.concat(rows,1)\n",
    "        return tf.image.encode_jpeg(tf.squeeze(image,[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}